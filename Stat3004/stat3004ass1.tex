\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{accents}
\usepackage{commath}
\usepackage{yfonts}
\usepackage{float}
\usepackage{array}

\title{Stat3004 Assignment 1}
\author{Dominic Scocchera}
\date{March 2023}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\begin{document}
\maketitle
\section*{Q1}
We want to show $\text{Var}(X)=\mathbb{E}[\text{Var}(X|Y)]+\text{Var}(\mathbb{E}[X|Y])$.
\begin{proof}
\begin{align*}
    \text{Var}(X)&=\mathbb{E}[X^2]-\mathbb{E}[X]^2\\
    &=\mathbb{E}[\mathbb{E}[X^2|Y]]-\mathbb{E}[X^2]\\
    &=\mathbb{E}[\text{Var}(X|Y)+\mathbb{E}[X|Y]^2]-\mathbb{E}[X^2]\\
    &=\mathbb{E}[\text{Var}(X|Y)+\mathbb{E}[X|Y]^2-\mathbb{E}[\mathbb{E}[X|Y]]]^2\\
    &=\mathbb{E}[\text{Var}(X|Y)]+(\mathbb{E}[\mathbb{E}[X|Y]^2]-\mathbb{E}[\mathbb{E}[X|Y]]^2)\\
    &=\mathbb{E}[\text{Var}(X|Y)]+\text{Var}(\mathbb{E}[X|Y])
\end{align*}
\end{proof}
\section*{Q2}
\subsection*{a)}
Let $X$ be a non-negative random variable with $p.d.f,\;\;f$. Show $\mathbb{E}X=\int_{0}^{\infty}\mathbb{P}(X\geq x)dx$.
\begin{proof}
\begin{align*}
\mathbb{E}X&=\int_{0}^{\infty}x f(x)dx\\
&=-\int_{0}^{\infty}x(-f(x))dx\\
&=-x(1-F(x))\Big\rvert_0^\infty+\int_{0}^{\infty}1-F(x)dx\;\;\;(*\text{note, Integration by parts and }F(x)\text{ is the }c.d.f)\\
&=\int_{0}^{\infty}1-F(x)dx\;\;\;(*\text{note }\lim_{x\rightarrow\infty}1-F(x)=0\;\;\;\text{and }-0(1-F(0))=0)\\
&=\int_{0}^{\infty}(1-\mathbb{P}(X\leq x))dx\\
&=\int_{0}^{\infty}\mathbb{P}(X\geq x)dx\\
\end{align*}
\end{proof}
\subsection*{b)}
Show $\mathbb{E}[x^\alpha]=\int_0^\infty\alpha x^{\alpha-1}\mathbb{P}(X\geq x)dx$, where $\alpha>0$.
\begin{proof}
\begin{align*}
\mathbb{E}[Y]&=\int_0^\infty\mathbb{P}(Y\geq y)dy\;\;\; (*\text{note we have this from a) and }Y=X^\alpha)\\
&=\int_0^\infty \alpha x^{\alpha-1}\mathbb{P}(X^\alpha\geq x^\alpha)dx\;\;\;(*\text{note the change of variable, }y=x^\alpha\iff dy=\alpha x^{\alpha-1}dx)\\
\end{align*}
\end{proof}
\section*{Q3}
Suppose $X_1,...,X_n$ are independent random variables with $c.d.f$'s $F_1,...,F_n$ respectively. Express the $c.d.f$ of $M=\min(X_1,...,X_n)$ in terms of the $\{F_i\}$.
\newline\newline
\begin{align*}
F_M(x)&=\mathbb{P}(M\leq x)\\
&=\mathbb{P}(\min(X_1,...,X_n)\leq x)\\
&=1-\mathbb{P}(X_1\geq x,...,X_n\geq x)\\
&=1-\mathbb{P}(X_1\geq x)...\mathbb{P}(X_n\geq x)\\
&=1-(1-F_1(x))...(1-F_n(x))\\
&=1-\prod_{i=1}^n(1-F_{i}(x))\\
\end{align*}
\section*{Q4}
\subsection*{a)}
Determine $\mathcal{G}(z)=\mathbb{E}\left[z^X\right]$ for $z\in[0,1]$.
\newline\newline
\begin{align*}
\mathcal{G}(z)&=\mathbb{E}\left[z^X\right]\\
&=z^0\mathbb{P}(X=0)+z^1\mathbb{P}(X=1)+z^2\mathbb{P}(X=2)\\
&=1-r-s+zr+z^2s\\
&=1+r(z-1)+s(z^2-1)\\
\end{align*}
\subsection*{b)}
We want to determine the mean and variance of $S_n$. We first determine the mean.
\begin{align*}
\mathbb{E}[S_n]&=\mathbb{E}\left[\sum_{i=1}^{S_{n-1}}X_{i,n-1}\right]\\
&=\mathbb{E}\left[\mathbb{E}\left[\sum_{i=1}^{S_{n-1}}X_{i,n-1}|S_{n-1}\right]\right]\\
&=\mathbb{E}\left[\sum_{i=1}^{S_{n-1}}\mathbb{E}\left[X_{i,n-1}|S_{n-1}\right]\right]\\
&=\mathbb{E}\left[\sum_{i=1}^{S_{n-1}}\mathbb{E}\left[X_{i,n-1}\right]\right]\\
&=\mathbb{E}[X_{i,n-1}]\mathbb{E}[S_{n-1}]\\
&=(0\cdot(1-r-s)+1\cdot r+2\cdot s)\mathbb{E}[S_{n-1}]\\
&=(r+2s)\mathbb{E}[S_{n-1}]\\
\end{align*}
We know $S_0=1\implies S_j=\mathbb{E}[X]^j$, so we have:
\begin{align*}
\mathbb{E}[S_n]&=(r+2s)(r+2s)^{n-1}\\
&=(r+2s)^n\\
\end{align*}
Now we determine the variance. First noting that $\mu=\mathbb{E}[X]=r+2s$ and  $\sigma^2=\text{Var}(X)=\mathbb{E}[X^2]-\mu^2=0^2(1-r-s)+1^2r+2^2s-(r+2s)^2=r+4s-(r+2s)^2$. And from lectures we have:
\begin{align*}
\text{Var}(S_n)&=\begin{cases}\sigma^2\mu^{n-1}\left(\frac{1-\mu^n}{1-\mu}\right), & \text{if } \mu\neq1\\\sigma^2 n, & \text{if } \mu=1\end{cases}\\
&=\begin{cases}(r+4s-(r+2s)^2)(r+2s)^{n-1}\left(\frac{1-(r+2s)^n}{1-(r+2s)}\right), & \text{if } r+2s\neq1\\(r+4s-(r+2s)^2) n, & \text{if } r+2s=1\end{cases}\\
\end{align*}
\subsection*{c)}
We know from lectures that the extinction probability is given by:
$$\eta=\mathcal{G}(\eta)$$
So we have:
\begin{align*}
\eta&=\mathcal{G}(\eta)\\
&=(1-r-s)+r\eta+s\eta^2\\
\end{align*}
Rearranging we get:
$$0=(1-r-s)+(r-1)\eta+s\eta^2$$
Solving for $\eta$ we get:
$$\eta=\begin{cases}
        \frac{-(r-1)-\sqrt{(r-1)^2-4s(1-r-s)}}{2s} & \text{if } r+2s>1\\
        1 & \text{if } r+2s\leq1\\
        0 & \text{if } r+s=1
    \end{cases}$$
The 0 case is trivial as we have that $\mathbb{P}(X=0)=1-r-s=1-(r+s)=1-1=0$, so there all individuals are guarenteed to reproduce and hence extinction is impossible. The 1 case follows from lectures, we have that $\mu=r+2s\leq1$ and from above we know the variance is non-zero in this case. The last case follows from the solution to the above equation. We also only use the negative sign case as the positive sign case is $\geq1$ which violates the requirement derived in lectures $(<1)$.
\begin{align*}
\frac{-(r-1)+\sqrt{(r-1)^2-4s(1-r-s)}}{2s}&\geq1\\
\sqrt{(r-1)^2-4s(1-r-s)}&\geq2s+(r-1)\\
(r-1)^2-4s(1-r-s)&\geq(2s+(r-1))^2\\
r^2-2r+1-4s+4sr+4s^2&\geq4s^2+4sr-4s+r^2-2r+1\\
0&\geq0\\
\end{align*}
Which always holds.  
\section*{Q5}
First we we want to show $\mathcal{G}_n(z)=\mathcal{G}_{n-1}(\mathcal{G}(z))$.
\begin{proof}
First we will show that for $X=Y_1+...+Y_N$ where $Y_i$ is $i.i.d$, noting that X is a random sum of random variables so N is a random variable, that $\mathcal{G}_X(z)=\mathcal{G}_N(\mathcal{G}_{Y_1}(z))$.
\begin{align*}
\mathcal{G}_X(z)&=\mathbb{E}[z^X]\\
&=\sum_{x=0}^{\infty}\mathbb{P}(X=x)z^x\\
&=\sum_{n=0}^{\infty}\sum_{x=0}^{\infty}\mathbb{P}(X=x|N=n)\mathbb{P}(N=n)z^x\\
&=\sum_{n=0}^{\infty}\mathbb{P}(N=n)\sum_{x=0}^{\infty}\mathbb{P}(X=x|N=n)z^x\\
&=\sum_{n=0}^{\infty}\mathbb{P}(N=n)(\mathcal{G}_{Y_1}(z))^n\\
&=\mathcal{G}_N(\mathcal{G}_{Y_1}(z))\\
\end{align*}
For a branching process we have $S_n=X_{1,n-1}+X_{2,n-1}+...+X_{N,n-1}$, where $X_{i,n-1}$ is the number of progeny produced by the $i_{\text{th}}$ member of the previous generation. It is clear to see that this is the same situation as what was shown above (random sum of random variables) and thus plugging in our variables to what was shown above we immediantly have our result:
$$\mathcal{G}_n(z)=\mathcal{G}_{n-1}(\mathcal{G}(z))$$
\end{proof}
\noindent Now we we want to show $\mathcal{G}_n(z)=\mathcal{G}(\mathcal{G}_{n-1}(z))$.
\begin{proof}
We can easily see from the above result that we can continue it to get $\mathcal{G}_{n-1}(z)=\mathcal{G}_{n-2}(\mathcal{G}(z))$. Continuing this until we reach $\mathcal{G}_2(z)=\mathcal{G}_{1}(\mathcal{G}(z))$. This holds as we have a random sum of random variables in each of these cases so this relation will hold regardless of the generation. We can see that $\mathcal{G}_n(z)=\mathcal{G}(...(\mathcal{G}(z)...)$ where this is n times. This is due to the recursive relation where for example $\mathcal{G}_3(z)=\mathcal{G}_{2}(\mathcal{G}(z))=\mathcal{G}_{1}(\mathcal{G}(\mathcal{G}(z)))$, we note here that $\mathcal{G}_1(z)=\mathcal{G}(z)$. From this we see that all the terms in the middle of the brackets on the right hand side equal $\mathcal{G}_{n-1}(z)$, so we have our result:
$$\mathcal{G}_n(z)=\mathcal{G}(\mathcal{G}_{n-1}(z))$$
\end{proof}
\end{document}